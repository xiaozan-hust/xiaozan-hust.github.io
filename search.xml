<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Hello World</title>
    <url>/2024/08/17/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>蜂鸟的第一篇文章</title>
    <url>/2024/08/17/%E8%9C%82%E9%B8%9F%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E6%96%87%E7%AB%A0/</url>
    <content><![CDATA[<p>蜂鸟的碎碎念：第一篇文章不知道写些什么，于是直接把自己之前写的一篇ROBOCON2024视觉组方案贴上去啦</p>
<h1 id="2024年”颗粒归仓”视觉组任务及方案"><a href="#2024年”颗粒归仓”视觉组任务及方案" class="headerlink" title="2024年”颗粒归仓”视觉组任务及方案"></a>2024年”颗粒归仓”视觉组任务及方案</h1><p><em><strong>written time:</strong></em> 2023&#x2F;11&#x2F;9</p>
<p><em><strong>points:</strong></em> 仅用于R2在三区的任务</p>
<h2 id="定位"><a href="#定位" class="headerlink" title="定位"></a>定位</h2><p><em><strong>ideas:</strong></em></p>
<ol>
<li>着重考虑在 <strong>谷仓区</strong> 的 <strong>绝对定位</strong> ，暂存区的绝对定位暂不考虑，一方面是暂存区存在球的遮挡，另一方面是暂存区对绝对定位的要求并不高。</li>
<li>暂不考虑使用 <strong>相对定位</strong> ，不过R2是在一区启动，如果存在 <strong>场地误差</strong> ，或许相对定位是一种更好的选择，或者考虑R2到达三区后进行一次 <strong>坐标修正</strong> （比如DT35）。</li>
</ol>
<h3 id="方案1：DT35-码盘定位"><a href="#方案1：DT35-码盘定位" class="headerlink" title="方案1：DT35+码盘定位"></a>方案1：DT35+码盘定位</h3><p><strong>设备：</strong> DT35若干</p>
<p><strong>思路：</strong> 每次R2到达谷仓区后，使用DT35对码盘数据进行修正。</p>
<p><strong>优点：</strong> 团队现有的方案，应用比较成熟。</p>
<p><strong>缺点：</strong></p>
<ol>
<li>由于码盘在上下坡过程中会出现误差，而DT35只能在谷仓区使用，所以该方案无法在暂存区进行精确的定位。</li>
<li>如果姿态（偏航角）不能调整很准确，DT35的测距信息会出现失真。</li>
</ol>
<h3 id="方案2：2D雷达-码盘定位"><a href="#方案2：2D雷达-码盘定位" class="headerlink" title="方案2：2D雷达+码盘定位"></a>方案2：2D雷达+码盘定位</h3><p><strong>设备：</strong> 思岚A3雷达</p>
<p><strong>思路：</strong> 每次R2到达谷仓区后，使用2D雷达扫描挡板-&gt;拟合出挡板-&gt;得出坐标信息，利用2D雷达的定位信息对码盘数据进行修正。</p>
<p><strong>优点：</strong> 对R2的姿态不做要求，理论上只要在谷仓区就可以得到坐标信息。</p>
<p><strong>缺点：</strong></p>
<ol>
<li>雷达帧率最高只有15hz，单用雷达不足以用于底盘的跑动。 <em>（可以考虑使用SLAM里程计的方法进行码盘和雷达的数据融合，参考EKF）</em></li>
<li>雷达数据会出现抖动（4mm左右），但是在用雷达数据刷新码盘数据的测试过程中却发现底盘的轨迹极其振荡，具体原因其实不太清楚。 <em>（可能一方面是雷达帧率太低，另一方面是雷达数据确实存在抖动）</em></li>
</ol>
<h3 id="方案3：开源SLAM算法"><a href="#方案3：开源SLAM算法" class="headerlink" title="方案3：开源SLAM算法"></a>方案3：开源SLAM算法</h3><p><strong>设备：</strong> MID360</p>
<p><strong>思路：</strong> 使用DLO, FAST_LIO2, LIO_SAM等开源算法，得到R2在整个赛场上的坐标信息，该方法可以在整个三区实现定位。</p>
<p><strong>优点：</strong> 工作量小，且可以实现在整个三区的定位。</p>
<p><strong>缺点：</strong> 必须要很熟悉算法的实现原理和代码实现过程，明确算法的优缺点，如果出了问题可以debug。</p>
<p><strong>实测情况：</strong></p>
<ol>
<li>目前进行测试了DLO和FAST_LIO2，DLO算法在MID360发布频率为10hz时与FAST_LIO2效果差不多，但是当MID360频率提到50hz时，DLO会出现很明显的振荡，基本上完全不能使用。</li>
<li>FAST_LIO2的数据在底盘跑动中精度和频率都还不错，且不会在上下坡的过程中出现误差累积，不过误差仍然是存在的，在SLAM中应该叫做 <strong>回程误差</strong> ，大约在2mm以内。不过现在来看会出现累积。 <em>（FAST_LIO2的论文中说在10hz的测试情况下，160m的距离下回程误差为7cm）</em></li>
</ol>
<h3 id="方案4：自写SLAM"><a href="#方案4：自写SLAM" class="headerlink" title="方案4：自写SLAM"></a>方案4：自写SLAM</h3><p><strong>设备：</strong> MID360, IMU, 磁编码器。</p>
<p><strong>思路：</strong> 采用动静态地图分离策略，离线构建静态地图并实时维护动态环境地图，基于多层地图策略实现动态环境下的准确的激光融合定位算法。</p>
<p><em><strong>tips:</strong></em></p>
<ol>
<li>静态地图指的是不包含任何场地模型之外的任何特征物的3D点云地图。</li>
<li>该方案其实是我的毕设，所以一定会去做，但实际上是否上场比赛有待测试。</li>
</ol>
<h3 id="个人想法"><a href="#个人想法" class="headerlink" title="个人想法"></a>个人想法</h3><p>截至目前（2023.11.16)，目前和控制组交流后的想法是使用 <em><strong>DT35+FAST_LIO2+码盘</strong></em> 进行数据融合定位，理论上可以实现在 <em><strong>整个三区的绝对定位</strong></em> ，且 <em><strong>理论精度高于三者的单独使用</strong></em> 。</p>
<h2 id="识别球"><a href="#识别球" class="headerlink" title="识别球"></a>识别球</h2><p><em><strong>ideas:</strong></em></p>
<ol>
<li>可以按照远近和动静进行分类，交叉组合就会有四种状态的球需要识别：近处静态球，近处动态球，远处静态球，远处动态球。</li>
<li>至少目前来看，要求识别精度要很高（个人觉得对精度要求低的方案更具有鲁棒性，才更适合ROBOCON）。</li>
<li>暂时考虑使用两个相机，远处的球使用高速相机，只需要的得到球的大致方位即可，近处的球使用深度相机，得到准确的球心坐标。</li>
</ol>
<h3 id="方案1：OpenCV-PCL"><a href="#方案1：OpenCV-PCL" class="headerlink" title="方案1：OpenCV+PCL"></a>方案1：OpenCV+PCL</h3><p><strong>设备：</strong> 高速相机+深度相机</p>
<p><strong>思路：</strong> 基于颜色对球进行提取，然后进行球的拟合和圆心至R2的坐标计算。</p>
<p><strong>评价：</strong> 该方案我目前交给吴圣标和黄子亮去做了，基本思路是在LAB色域将球大致提取出来-&gt;Canny边缘检测-&gt;考虑高光补偿-&gt;霍夫圆拟合-&gt;筛选出优圆-&gt;结合深度图进行坐标转换-&gt;得到球心坐标。</p>
<p>个人觉得这套方案有两个点需要着重注意：</p>
<ol>
<li>比较依赖于在LAB色域对球的提取，如果效果不好将会对后续的算法产生较大的影响。</li>
<li>在Canny边缘检测后进行霍夫圆拟合，往往会由于一些噪点拟合出很多霍夫圆，噪点不可能完全避免，所以比较依赖于霍夫圆的筛选，而霍夫圆的筛选条件是否可以应对各种情况呢。</li>
</ol>
<h3 id="方案2：YOLOv5-深度图"><a href="#方案2：YOLOv5-深度图" class="headerlink" title="方案2：YOLOv5+深度图"></a>方案2：YOLOv5+深度图</h3><p><strong>设备：</strong> 高速相机+深度相机</p>
<p><strong>思路：</strong> 首先利用YOLO对球进行识别，然后将识别框内对应的点云图进行球的拟合和圆心至R2的坐标计算。</p>
<p><strong>评价：</strong> 目前（2023.11.16）算是已经基本完成的方案，帧率可以把深度相机的30帧跑满，精度大概1cm，不过还没有经过更为充分的测试。</p>
<h3 id="个人想法-1"><a href="#个人想法-1" class="headerlink" title="个人想法"></a>个人想法</h3><p>我觉得YOLO相比于OpenCV是具有更高的鲁棒性的，其实两种方案都是可以试一下的，这周末球和框到了之后，可以做一个更好的数据集进行测试。</p>
<h2 id="识别谷仓"><a href="#识别谷仓" class="headerlink" title="识别谷仓"></a>识别谷仓</h2><p><em><strong>ideas:</strong></em></p>
<ol>
<li>相机要有足够的视角，必须能够同时看到五个框。</li>
<li>要考虑到对方三区的球和R2的干扰。</li>
</ol>
<h3 id="方案1：YOLOv5"><a href="#方案1：YOLOv5" class="headerlink" title="方案1：YOLOv5"></a>方案1：YOLOv5</h3><p><strong>设备：</strong> 高速相机</p>
<p><strong>思路：</strong> 创建谷仓的数据集，利用YOLO实现对谷仓的识别，然后只识别谷仓内球的数量和颜色。</p>
<p><em><strong>tips:</strong></em> 该方法可以避免对方三区的球和R2的干扰，但是前提是YOLO识别谷仓的效果足够好。</p>
<p><strong>优点：</strong> 无需更改设备，可以延续使用识别球的高速相机+深度相机。</p>
<p><strong>缺点：</strong> 需要测试谷仓的识别情况，如果谷仓识别效果不能满足比赛需要，则该方案pass。</p>
<h3 id="方案2：直通点云拟合"><a href="#方案2：直通点云拟合" class="headerlink" title="方案2：直通点云拟合"></a>方案2：直通点云拟合</h3><p><strong>设备：</strong> 深度相机</p>
<p><strong>思路：</strong> 利用深度相机或者激光雷达（雷达大概率点云太少），结合定位信息，过滤出在谷仓附近的点云，然后进行拟合（精度要求不高，只需要判断出有几个球即可），甚至不用判断球的颜色，因为己方的球是已知的，如果检测出了多的球，绝对是对方的球。</p>
<p><em><strong>tips:</strong></em> 深度相机的视角大概率不能够看全五个谷仓，所以可能需要上多个深度相机。</p>
<p><strong>缺点：</strong> 需要额外考虑添加设备，且点云的处理可能不会很快。</p>
<h3 id="个人想法-2"><a href="#个人想法-2" class="headerlink" title="个人想法"></a>个人想法</h3><p>如果YOLO能够对谷仓进行比较好的识别，我觉得优先选用方案1比较合适。</p>
<p>还有值得说的一点是，球和谷仓的数据集可以做在一起，对于谷仓，可以包含内有球和无球的两种情况；对于球，可以包含在地上和在谷仓内两种情况。</p>
<p>另外，最好三区谷仓区后面可以用一块挡板或者镜子，因为在制作数据集时需要将环境也制作在数据集内，团队谷仓后的货架绝对在正式比赛的场地中是看不到的，该部分信息不应包含在数据集内。</p>
]]></content>
      <categories>
        <category>教程</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>教程</tag>
        <tag>尝试</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLOv5使用教程</title>
    <url>/2024/08/17/YOLOv5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/</url>
    <content><![CDATA[<h1 id="YOLOv5：一夜暴富"><a href="#YOLOv5：一夜暴富" class="headerlink" title="YOLOv5：一夜暴富"></a>YOLOv5：一夜暴富</h1><h2 id="1-关于YOLO"><a href="#1-关于YOLO" class="headerlink" title="1.关于YOLO"></a>1.关于YOLO</h2><h3 id="1-1-目标检测的概念与发展"><a href="#1-1-目标检测的概念与发展" class="headerlink" title="1.1 目标检测的概念与发展"></a>1.1 目标检测的概念与发展</h3><p>目标检测，也叫做目标提取，是一种基于目标几何和统计特征的图像分割。它将目标的分割和识别合二为一。在复杂的应用场景中，需要同时考虑对多个目标进行实时检测，其准确性和实时性是整个系统的一项重要能力。</p>
<p>随着计算机技术的发展和计算机视觉原理的广泛应用，利用计算机图像处理技术对目标进行实施跟踪研究越来越热门，对目标进行动态实施跟踪定位在智能化交通系统，智能监控系统，军事目标检测及医学导航手术中手术器械定位等方面具有广泛的应用价值。</p>
<h3 id="1-2-YOLO的发展与优势"><a href="#1-2-YOLO的发展与优势" class="headerlink" title="1.2 YOLO的发展与优势"></a>1.2 YOLO的发展与优势</h3><p><strong>YOLO（You Only Look Once）</strong>是一种用于实时目标检测的深度学习模型。</p>
<p>YOLO最初的创造者维护了YOLOv1，YOLOv2，YOLOv3。YOLOv4是对YOLOv3的一个改进，YOLOv4的作者与前三个版本YOLO的作者并不是同一人。YOLOv5在YOLOv4发布不久后出现，由Glenn Jocher使用Pytorch框架建立。</p>
<ul>
<li><p><em><strong>YOLO v1（2016）：</strong></em></p>
<ul>
<li>YOLO v1是YOLO系列的第一个版本，由Joseph Redmon等人于2016年提出。它采用单一神经网络，将目标检测问题转化为回归问题。YOLO v1将输入图像分成较小的网格，并为每个网格单独预测边界框（bounding box）和相应的类别概率分数。</li>
<li>YOLO v1的优点是速度快，可以实现实时目标检测，但它在检测小目标和处理重叠的目标方面存在一些问题。</li>
</ul>
</li>
<li><p><em><strong>YOLO v2 &#x2F; YOLO9000（2017）</strong>：</em></p>
<ul>
<li>YOLO v2在YOLO v1的基础上进行了改进，采用了更深的神经网络，称为Darknet-19。</li>
<li>YOLO9000是YOLO v2的扩展版本，支持更多的目标类别。它通过引入WordTree等技术，实现了对超过9000个不同类别的目标进行检测。</li>
</ul>
</li>
<li><p><em><strong>YOLO v3（2018）：</strong></em></p>
<ul>
<li>YOLO v3是YOLO系列的下一个版本，引入了多尺度预测。它使用了三个不同尺度的特征图来检测不同大小的目标。</li>
<li>YOLO v3改进了目标检测的精度和稳定性，成为一种强大的目标检测算法。</li>
</ul>
</li>
<li><p><em><strong>YOLO v4（2020）：</strong></em></p>
<ul>
<li>YOLO v4是YOLO系列的又一重大改进，由Alexey Bochkovskiy等人开发。它引入了一系列技术，如CIOU损失函数、PANet、SAM块等，以提高目标检测的性能。</li>
<li>YOLO v4具有更高的准确性和速度，成为了目标检测领域的领先算法之一。</li>
</ul>
</li>
<li><p><em><strong>YOLO v5（2020）：</strong></em></p>
<ul>
<li>YOLO v5由Ultralytics团队于2020年发布。它着重于轻量化和速度优化，并且具有高度可配置性。YOLO v5的不同版本（如Yolov5s、Yolov5m、Yolov5l和Yolov5x）可以根据需求进行选择。</li>
<li>YOLO v5在速度和性能之间取得了很好的平衡，适用于各种实时目标检测应用。</li>
</ul>
</li>
</ul>
<p><em><strong>YOLOv1&#x2F;v2&#x2F;v3:</strong></em></p>
<pre><code>源码：[https://github.com/pjreddie/darknet](YOLOv1/2/3源码)

论文：[https://pjreddie.com/](YOLOv1/2/3论文)
</code></pre>
<p><em><strong>YOLOv4：</strong></em></p>
<pre><code>源码：[https://github.com/AlexeyAB/darknet](YOLOv4源码)

论文：[https://arxiv.org/abs/2004.10934](YOLOv4论文)
</code></pre>
<p><em><strong>YOLOv5:</strong></em></p>
<pre><code>源码：[https://github.com/ultralytics/yolov5](YOLOv5源码)
</code></pre>
<p><strong>YOLOv5的优势：</strong></p>
<ul>
<li>基于Pytorch框架，相对于Darknet框架它的扩展性更强。</li>
<li>源码使用PYTHON，使得代码更加便于修改，非常适合使用需求。</li>
<li>代码量更小，适合阅读，借鉴学习。</li>
<li>更加优秀的性能表现。</li>
</ul>
<p><strong>YOLOv6及更高版本：</strong></p>
<ul>
<li><em><strong>YOLOv6:</strong></em> 由美团视觉研发部（Meituan Vision AI Department）于2022年开源，并应用于该公司的许多自动送货机器人中。</li>
<li><em><strong>YOLOv7:</strong></em> 添加了额外的任务，例如COCO关键点数据集上的姿态估计。</li>
<li><em><strong>YOLOv8:</strong></em> 是UItralytics推出的YOLO最新版本。作为一种最尖端，最先进的（SOTA）模型，YOLOv8建立在先前版本的成功的基础上，引入了许多新功能和改进，以增强性能，灵活性和效率。YOLOv8支持全方位的视觉AI任务，包括<em><strong>检测，分割，姿态估计，跟踪和分类</strong></em>。这种多功能性允许用户在不同的应用程序和领域中利用YOLOv8的功能。</li>
</ul>
<h2 id="2-开发环境搭建"><a href="#2-开发环境搭建" class="headerlink" title="2.开发环境搭建"></a>2.开发环境搭建</h2><h3 id="2-1-前期环境准备"><a href="#2-1-前期环境准备" class="headerlink" title="2.1 前期环境准备"></a>2.1 前期环境准备</h3><h4 id="2-1-2-关于PyTorch"><a href="#2-1-2-关于PyTorch" class="headerlink" title="2.1.2 关于PyTorch"></a>2.1.2 关于PyTorch</h4><p>PyTorch是一个早期的深度学习框架，是深度学习领域的先驱之一。</p>
<p>PyTorch有三个子库，安装时需要单独安装，分别为：</p>
<ul>
<li><em><strong>torch:</strong></em> PyTorch核心库，提供了深度学习框架的基本功能，可用于各种深度学习任务，包括计算机视觉，自然语言处理和强化学习等。</li>
<li><em><strong>torchvison:</strong></em> PyTorch计算机视觉库，专门用于处理图像和视觉数据。包括图像数据加载器，常用的数据集，预训练的模型和图像处理工具。</li>
<li><em><strong>torchaudio:</strong></em> PyTorch的音频处理库，用于处理音频数据。包括音频数据加载器，音频特征提取工具，常用的音频数据集和用于处理声音的函数和模型。</li>
</ul>
<h4 id="2-1-2-搭建YOLOv5的预安装环境"><a href="#2-1-2-搭建YOLOv5的预安装环境" class="headerlink" title="2.1.2 搭建YOLOv5的预安装环境"></a>2.1.2 搭建YOLOv5的预安装环境</h4><p>官方给出的YOLOv5的安装前提要求是：<strong>Python &gt;&#x3D; 3.8.0 &amp;&amp; PyTorch &gt;&#x3D;1.8</strong></p>
<p><strong>关于Python的安装</strong>: <a href="Python%E5%AE%98%E7%BD%91">https://www.python.org/</a></p>
<p><strong>关于PyTorch的安装</strong>：<a href="Pytorch%E7%9A%84%E5%AE%89%E8%A3%85">https://pytorch.org/</a></p>
<p>当然，我自己比较满意的一个开发环境应该是：</p>
<ul>
<li>Ubuntu20.04及以上</li>
<li>预先安装好ROS-neotic</li>
<li>使用 <code>pip3 install torch torchvision torchaudio</code>安装PyTorch</li>
</ul>
<p>当Ubuntu系统为20.04及以上，此时电脑的Python版本 &gt;&#x3D; 3.8，已经满足YOLOv5对于Python版本的要求。对于Pytorch版本，虽然很多人的一贯想法都是只要版本够用就不要用最新版，但是我之前一开始就是装的最新版，而且截至到目前也一直没有出现过问题，所以我觉得如果不想折腾，也不用去上面的官网去找版本下载，直接用pip3安装（<em><strong>这里值得一说的是，当python版本为python3时，建议都用pip3安装，而不要再用pip，不同的安装方式会将库装在不同的python文件夹，当使用python3去执行程序时，如果依赖的库被装在了python2的文件夹中，那么程序是无法使用这个库的</strong></em>）。</p>
<h3 id="2-2-YOLOv5的安装"><a href="#2-2-YOLOv5的安装" class="headerlink" title="2.2 YOLOv5的安装"></a>2.2 YOLOv5的安装</h3><p>YOLOv5的安装其实非常简单：</p>
<ul>
<li><em><strong>下载仓库到本地：</strong></em> <code>git clone https://github.com/ultralytics/yolov5</code></li>
<li><em><strong>进入yolov5文件夹：</strong></em> <code>cd yolov5</code></li>
<li><em><strong>安装所需依赖：</strong></em> <code>pip install -r requirements.txt</code></li>
</ul>
<p>至此，你的计算机现在已经可以使用YOLOv5了，可以通过在yolov5文件夹打开终端运行python3 detect.py，该程序将会利用已经训练好的模型对&#x2F;data&#x2F;images&#x2F;下的图片进行目标检测，并且在&#x2F;runs&#x2F;detect&#x2F;exp文件夹查看检测后的结果。</p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/1694479671537.png" alt="F4"></p>
<h3 id="2-3-GPU环境搭建及YOLOv5的使用"><a href="#2-3-GPU环境搭建及YOLOv5的使用" class="headerlink" title="2.3 GPU环境搭建及YOLOv5的使用"></a>2.3 GPU环境搭建及YOLOv5的使用</h3><p><strong>！！！！！！！！！！！！！！！安装过程中，全程不要挂梯子，并且退出后台。！！！！！！！！！！！！！！！！！！！！！！！！！！</strong></p>
<h4 id="2-3-1-显卡驱动安装"><a href="#2-3-1-显卡驱动安装" class="headerlink" title="2.3.1 显卡驱动安装"></a>2.3.1 显卡驱动安装</h4><p>安装显卡驱动可以使用命令行安装，但是有一种比较方便的方式进行安装：</p>
<ul>
<li>进入软件与更新–&gt;附加驱动–&gt;选择对应版本的驱动</li>
<li>应用更改后重启电脑</li>
<li>终端输入nvidia-smi即可查看驱动信息</li>
</ul>
<p>例如我所选的驱动版本为：</p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/02.png" alt="驱动"></p>
<p>显示信息为：<br><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/03.png" alt="smi"></p>
<p>此外，可以看到，终端还显示了推荐的CUDA版本，可以选择推荐的版本，也可以自行选择版本安装。</p>
<h4 id="2-3-2-CUDA安装"><a href="#2-3-2-CUDA安装" class="headerlink" title="2.3.2 CUDA安装"></a>2.3.2 CUDA安装</h4><p><a href="https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html">显卡驱动与CUDA版本对应</a></p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/04.png" alt="CUDA与驱动版本"></p>
<p><a href="https://developer.nvidia.com/cuda-toolkit-archive">CUDA版本选择与安装步骤</a></p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/05.png" alt="CUDA安装"></p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/06.png" alt="CUDA安装"></p>
<p>根据自己的显卡驱动版本选择对应的CUDA版本，推荐不要下载太低版本的CUDA，因为后面的Pytorch对CUDA的版本也有要求，太低的CUDA版本可能导致你下载的Pytorch并不能满足比如yolov5对最低版本Pytorch的要求。</p>
<p><em><strong>tips:</strong></em> 在安装的过程中，很大概率会在安装到99%时出现段错误（核心已转储），如下图所示：</p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/08.png" alt="段错误"></p>
<p>原因是栈太小，解决方法如下：</p>
<ul>
<li>查看栈大小：ulimit -a</li>
<li>更改stack size：ulimit -s 102400</li>
</ul>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/09.png" alt="段错误"></p>
<p><a href="https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#ubuntu-x86_64">添加环境变量</a></p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/07.png" alt="环境变量"></p>
<p>以上内容(注意改成自己版本的CUDA)在~&#x2F;.bashrc尾行中添加。</p>
<p><em><strong>完成以上内容后，在终端输入：</strong></em></p>
<ul>
<li><em>nvcc -V</em></li>
</ul>
<p><em><strong>查看是否安装成功。</strong></em></p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/10.png" alt="查看">\</p>
<p><strong>！！！！！！！安装完之后最好重启一次电脑，以验证是否能正确加载nvidia驱动，如果不能正确加载，则手动再装一次即可！！！！！！！！！！！！！</strong></p>
<h4 id="2-3-3-安装torch"><a href="#2-3-3-安装torch" class="headerlink" title="2.3.3 安装torch"></a>2.3.3 安装torch</h4><p><a href="https://pytorch.org/get-started/previous-versions/">torch与CUDA版本对应及下载</a></p>
<p>由于我装的CUDA为12.0，查看了一下，官网上没有直接支持cuda 12的torch版本，但是翻阅社区了解到，cuda是向下兼容的，cuda 12可以支持官网里面的CUDA 11.8。</p>
<p>可以使用conda安装，也可以使用pip安装，我在这选择的是使用pip安装，并且将官网上给出的pip改为pip3，读者可自行去了解二者的不同。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip3 install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 --index-url https://download.pytorch.org/whl/cu118</span><br></pre></td></tr></table></figure>

<h4 id="2-3-4-验证安装结果"><a href="#2-3-4-验证安装结果" class="headerlink" title="2.3.4 验证安装结果"></a>2.3.4 验证安装结果</h4><p>依次在终端输入：</p>
<ul>
<li>python3</li>
<li>import torch</li>
<li>print(torch.cuda.is_available())</li>
</ul>
<p>返回true代表安装成功。</p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/11.png" alt="查看"></p>
<p>安装成功但是warnning:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">WARNING: The script isympy is installed in &#x27;/home/jiu/.local/bin&#x27; which is not on PATH.</span><br><span class="line">Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</span><br><span class="line">WARNING: The scripts cmake, cpack and ctest are installed in &#x27;/home/jiu/.local/bin&#x27; which is not on PATH.</span><br><span class="line">Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</span><br><span class="line">WARNING: The script lit is installed in &#x27;/home/jiu/.local/bin&#x27; which is not on PATH.</span><br><span class="line">Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</span><br><span class="line">WARNING: The scripts convert-caffe2-to-onnx, convert-onnx-to-caffe2 and torchrun are installed in &#x27;/home/jiu/.local/bin&#x27; which is not on PATH.</span><br><span class="line">Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</span><br><span class="line">WARNING: The scripts f2py, f2py3 and f2py3.8 are installed in &#x27;/home/jiu/.local/bin&#x27; which is not on PATH.</span><br><span class="line">Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>hahahh</p>
<h3 id="2-4-cuDNN与TensorRT"><a href="#2-4-cuDNN与TensorRT" class="headerlink" title="2.4 cuDNN与TensorRT"></a>2.4 cuDNN与TensorRT</h3><h4 id="2-4-1-cuDNN"><a href="#2-4-1-cuDNN" class="headerlink" title="2.4.1 cuDNN"></a>2.4.1 cuDNN</h4><p>NVIDIA CUDA 深度神经网络 （cuDNN） 是用于深度神经网络的 GPU 加速基元库</p>
<p><a href="https://blog.csdn.net/h3c4lenovo/article/details/119003405">参考博客</a></p>
<p><a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html">官方安装指引</a></p>
<p><a href="https://developer.nvidia.com/rdp/cudnn-archive">官方下载</a><br><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/12.png" alt="查看"></p>
<p>下载对应CUDA的版本就好了，我的CUDA是12.0.0，所以我下载的是cuDNN8.9.0 .</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xvf cudnn-linux-x86_64-8.9.0.131_cuda12-archive.tar.xz</span><br><span class="line">sudo cp cudnn-linux-x86_64-8.9.0.131_cuda12-archive/include/cudnn*.h /usr/local/cuda/include </span><br><span class="line">sudo cp -P cudnn-linux-x86_64-8.9.0.131_cuda12-archive/lib/libcudnn* /usr/local/cuda/lib64 </span><br><span class="line">sudo chmod a+r /usr/local/cuda/include/cudnn*.h /usr/local/cuda/lib64/libcudnn*  </span><br></pre></td></tr></table></figure>

<p>查看是否安装完毕：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cat /usr/local/cuda/include/cudnn_version.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure>

<h4 id="2-4-2-TensorRT"><a href="#2-4-2-TensorRT" class="headerlink" title="2.4.2 TensorRT"></a>2.4.2 TensorRT</h4><p><a href="https://blog.csdn.net/weixin_43863869/article/details/128571567">参考博客</a></p>
<p><a href="https://developer.nvidia.com/nvidia-tensorrt-8x-download">官网下载地址</a></p>
<p><a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html">官方安装指引</a></p>
<p>这里注意最新版的TensorRT也只支持CUDA12.1 。<strong>所以CUDA不要超过12.1，例如我装的就是12.0.0</strong></p>
<p><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/13.png" alt="查看"></p>
<p>我用的是Tar安装，版本是TensorRT8.6 。用tar安装的好处是，把解压好的文件和.bashrc里面的路径删除就可以很方便的删除TensorRT了。</p>
<p>下载好之后，将这个移动到opt文件夹中： sudo mv TensorRT-8.6.1.6.Linux.x86_64-gnu.cuda-12.0.tar.gz &#x2F;opt 方便管理</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">tar -xzvf TensorRT-$&#123;version&#125;.Linux.$&#123;arch&#125;-gnu.$&#123;cuda&#125;.tar.gz</span><br><span class="line">cd TensorRT-$&#123;version&#125;/python</span><br><span class="line">python3 -m pip install tensorrt-*-cp3x-none-linux_x86_64.whl</span><br><span class="line">python3 -m pip install tensorrt_lean-*-cp3x-none-linux_x86_64.whl</span><br><span class="line">python3 -m pip install tensorrt_dispatch-*-cp3x-none-linux_x86_64.whl</span><br><span class="line">cd TensorRT-$&#123;version&#125;/uff</span><br><span class="line">python3 -m pip install uff-0.6.9-py2.py3-none-any.whl</span><br><span class="line">cd TensorRT-$&#123;version&#125;/graphsurgeon</span><br><span class="line">python3 -m pip install graphsurgeon-0.4.6-py2.py3-none-any.whl</span><br><span class="line">cd TensorRT-$&#123;version&#125;/onnx_graphsurgeon</span><br><span class="line">python3 -m pip install onnx_graphsurgeon-0.3.12-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure>

<p>照着官方安装都安装一遍就可以了 <em><strong>注意将命令中的x和星号改成自己需要的版本</strong></em> 。</p>
<p>官方给的在bashrc里面的路径是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:&lt;TensorRT-$&#123;version&#125;/lib&gt;</span><br></pre></td></tr></table></figure>

<p>到我这里就是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/TensorRT-8.6.1.6/lib</span><br></pre></td></tr></table></figure>

<p>再source ~&#x2F;.bashrc刷新一下，TensorRT就算装好了</p>
<h3 id="2-5-支持CUDA的opencv"><a href="#2-5-支持CUDA的opencv" class="headerlink" title="2.5 支持CUDA的opencv"></a>2.5 支持CUDA的opencv</h3><p><a href="https://blog.csdn.net/u011622208/article/details/104265206#commentBox">参考博客</a></p>
<p>因为CUDA的版本很高，所以我的opencv也尽量装的高。我装的是：OpenCV 4.8.1</p>
<p>官网上把压缩包下下来，然后：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_INSTALL_PREFIX=/usr/local -D CMAKE_BUILD_TYPE=Release -D OPENCV_GENERATE_PKGCONFIG=ON -D ENABLE_CXX11=1 -D OPENCV_ENABLE_NONFREE=True ..</span><br><span class="line"></span><br><span class="line">cmake -D CMAKE_INSTALL_PREFIX=/usr/local -D CMAKE_BUILD_TYPE=Release -D OPENCV_GENERATE_PKGCONFIG=ON -D ENABLE_CXX11=1 -D OPENCV_EXTRA_MODULES_PATH=../opencv_contrib/modules -D OPENCV_ENABLE_NONFREE=True -D INSTALL_PYTHON_EXAMPLES=ON -D INSTALL_C_EXAMPLES=ON -D WITH_CUDA=ON -D WITH_TBB=ON -D ENABLE_FAST_MATH=1 -D WITH_OPENMP=ON -D WITH_CUFFT=ON -D WITH_CUBLAS=ON ..</span><br><span class="line"></span><br><span class="line">make -j12</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>

<h2 id="3-训练自己的数据集"><a href="#3-训练自己的数据集" class="headerlink" title="3.训练自己的数据集"></a>3.训练自己的数据集</h2><h3 id="3-1-数据集拆分"><a href="#3-1-数据集拆分" class="headerlink" title="3.1 数据集拆分"></a>3.1 数据集拆分</h3><p>将标好的数据集划分为训练集和测试集，get_train_valid.py代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">import os</span><br><span class="line">import random</span><br><span class="line">import shutil</span><br><span class="line">import copy</span><br><span class="line"></span><br><span class="line">path_root_labels = &#x27;./data_11_24/labels&#x27;    # 数据集标签路径</span><br><span class="line">path_root_imgs = &#x27;./data_11_24/images&#x27;      # 数据集图片路径</span><br><span class="line">path_splited = &#x27;./data_split&#x27;               # 数据集划分路径</span><br><span class="line"></span><br><span class="line">if not os.path.exists(path_splited):</span><br><span class="line">    os.mkdir(path_splited)</span><br><span class="line">    os.mkdir(os.path.join(path_splited, &#x27;images&#x27;))</span><br><span class="line">    os.mkdir(os.path.join(path_splited, &#x27;labels&#x27;))</span><br><span class="line">    os.mkdir(os.path.join(path_splited, &#x27;images/train&#x27;))</span><br><span class="line">    os.mkdir(os.path.join(path_splited, &#x27;images/valid&#x27;))</span><br><span class="line">    os.mkdir(os.path.join(path_splited, &#x27;labels/train&#x27;))</span><br><span class="line">    os.mkdir(os.path.join(path_splited, &#x27;labels/valid&#x27;))</span><br><span class="line"></span><br><span class="line">file_list = os.listdir(path_root_imgs)</span><br><span class="line">origin = copy.deepcopy(file_list)</span><br><span class="line">random.shuffle(file_list)</span><br><span class="line">train_len = int(0.9 * len(file_list))   # 训练集占比</span><br><span class="line"></span><br><span class="line">for i, file in enumerate(file_list):</span><br><span class="line">    image_label = file.split(&#x27;.&#x27;)[0] + &#x27;.txt&#x27;</span><br><span class="line">    label_content = &quot;&quot;</span><br><span class="line"></span><br><span class="line">    if i &lt; train_len:</span><br><span class="line">        img_path = &#x27;./data_split/images/train&#x27;</span><br><span class="line">        label_path = &#x27;./data_split/labels/train&#x27;</span><br><span class="line">    else:</span><br><span class="line">        img_path = &#x27;./data_split/images/valid&#x27;</span><br><span class="line">        label_path = &#x27;./data_split/labels/valid&#x27;</span><br><span class="line"></span><br><span class="line">    # 复制图像文件至划分数据集文件夹</span><br><span class="line">    shutil.copy(os.path.join(path_root_imgs, file), img_path)</span><br><span class="line"></span><br><span class="line">    # 复制标签文件至划分数据集文件夹</span><br><span class="line">    label_file_path = os.path.join(path_root_labels, image_label)</span><br><span class="line">    if os.path.exists(label_file_path):</span><br><span class="line">        shutil.copy(label_file_path, label_path)</span><br><span class="line">    else:</span><br><span class="line">        # 如果标签文件不存在，创建空文件</span><br><span class="line">        with open(os.path.join(label_path, image_label), &#x27;w&#x27;) as empty_label_file:</span><br><span class="line">            empty_label_file.write(label_content)</span><br><span class="line"></span><br><span class="line">file_list = os.listdir(&#x27;./data_11_24/images&#x27;)</span><br><span class="line">print(len(file_list))</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>该代码会自动创建划分后的数据集文件夹，对于没有标签的背景图片会创建一个对应的空白标签。空白标签同样可以用于YOLOv5数据集的训练。</p>
<h3 id="3-2-创建数据参数文件"><a href="#3-2-创建数据参数文件" class="headerlink" title="3.2 创建数据参数文件"></a>3.2 创建数据参数文件</h3><p>在data&#x2F;文件夹下创建，例如创建名为ball.yaml的参数文件如下（格式可以从已有的参数文件中复制）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># YOLOv5 🚀 by Ultralytics, GPL-3.0 license</span><br><span class="line"># COCO 2017 dataset http://cocodataset.org</span><br><span class="line"># Example usage: python train.py --data coco.yaml</span><br><span class="line"># parent</span><br><span class="line"># ├── yolov5</span><br><span class="line"># └── datasets</span><br><span class="line">#     └── coco  ← downloads here</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Train/val/test sets as 1) dir: path/to/imgs, 2) file: path/to/imgs.txt, or 3) list: [path/to/imgs1, path/to/imgs2, ..]</span><br><span class="line">path: /home/jxj/yolov5-6.0/dataset  # 数据集文件夹父目录</span><br><span class="line">train: train_data/images  # 训练集图像位置（会在该同级目录下自动搜寻对应的标签文件）</span><br><span class="line">val: valid_data/images  # 测试集图像位置（会在该同级目录下自动搜寻对应的标签文件）</span><br><span class="line">test:   # 20288 of 40670 images, submit to https://competitions.codalab.org/competitions/20794</span><br><span class="line"></span><br><span class="line"># Classes</span><br><span class="line">nc: 4  # 标签数量</span><br><span class="line">names: [&#x27;purple_box&#x27;,&#x27;blue_box&#x27;,&#x27;red_box&#x27;,&#x27;granary&#x27;]  # 标签名称（与标注工具中一致）</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>要修改的只有有中文注释的那部分。</p>
<p>需要注意的是，该数据集的文件夹架构与前面用get_train_valid.py创建的文件夹架构是不一样的，需要手动将相对应的文件复制过来，或者修改一下get_train_valid.py代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">yolov5                                      |	yolov5</span><br><span class="line">|——data_split          			    |	|——dataset</span><br><span class="line">  |——images				    |	  |——train_data</span><br><span class="line">    |——train				    |	    |——images</span><br><span class="line">      |——rgb_0.png			    |	      |——rgb_0.png</span><br><span class="line">      ...				    |	      ...</span><br><span class="line">    |——valid				    |	    |——labels</span><br><span class="line">      |——rgb_1.png			    |	      |——rgb_0.txt						  </span><br><span class="line">      ...				    |	      ...</span><br><span class="line">  |——labels				    |	  |——valid_data</span><br><span class="line">    |——train				    |	    |——images</span><br><span class="line">      |——rgb_0.txt			    |	      |——rgb_1.png</span><br><span class="line">      ...				    |	      ...</span><br><span class="line">    |——valid				    |	    |——labels</span><br><span class="line">      |——rgb_1.txt			    |	      |——rgb_1.txt</span><br><span class="line">      ...				    |	      ...</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="3-3-创建模型参数文件"><a href="#3-3-创建模型参数文件" class="headerlink" title="3.3 创建模型参数文件"></a>3.3 创建模型参数文件</h3><p>在models&#x2F;文件夹下创建，例如创建名为yolov5s_ball.yaml的参数文件如下（先自行选择一个模型，比如yolov5s或者yolov5l，然后以该文件为基础进行修改）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># YOLOv5 🚀 by Ultralytics, GPL-3.0 license</span><br><span class="line"></span><br><span class="line"># Parameters</span><br><span class="line">nc: 4  # 标签数量</span><br><span class="line">depth_multiple: 0.33  # model depth multiple</span><br><span class="line">width_multiple: 0.50  # layer channel multiple</span><br><span class="line">anchors:</span><br><span class="line">  - [10,13, 16,30, 33,23]  # P3/8</span><br><span class="line">  - [30,61, 62,45, 59,119]  # P4/16</span><br><span class="line">  - [116,90, 156,198, 373,326]  # P5/32</span><br><span class="line"></span><br><span class="line"># YOLOv5 v6.0 backbone</span><br><span class="line">backbone:</span><br><span class="line">  # [from, number, module, args]</span><br><span class="line">  [[-1, 1, Conv, [64, 6, 2, 2]],  # 0-P1/2</span><br><span class="line">   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4</span><br><span class="line">   [-1, 3, C3, [128]],</span><br><span class="line">   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8</span><br><span class="line">   [-1, 6, C3, [256]],</span><br><span class="line">   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16</span><br><span class="line">   [-1, 9, C3, [512]],</span><br><span class="line">   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32</span><br><span class="line">   [-1, 3, C3, [1024]],</span><br><span class="line">   [-1, 1, SPPF, [1024, 5]],  # 9</span><br><span class="line">  ]</span><br><span class="line"></span><br><span class="line"># YOLOv5 v6.0 head</span><br><span class="line">head:</span><br><span class="line">  [[-1, 1, Conv, [512, 1, 1]],</span><br><span class="line">   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],</span><br><span class="line">   [[-1, 6], 1, Concat, [1]],  # cat backbone P4</span><br><span class="line">   [-1, 3, C3, [512, False]],  # 13</span><br><span class="line"></span><br><span class="line">   [-1, 1, Conv, [256, 1, 1]],</span><br><span class="line">   [-1, 1, nn.Upsample, [None, 2, &#x27;nearest&#x27;]],</span><br><span class="line">   [[-1, 4], 1, Concat, [1]],  # cat backbone P3</span><br><span class="line">   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)</span><br><span class="line"></span><br><span class="line">   [-1, 1, Conv, [256, 3, 2]],</span><br><span class="line">   [[-1, 14], 1, Concat, [1]],  # cat head P4</span><br><span class="line">   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)</span><br><span class="line"></span><br><span class="line">   [-1, 1, Conv, [512, 3, 2]],</span><br><span class="line">   [[-1, 10], 1, Concat, [1]],  # cat head P5</span><br><span class="line">   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)</span><br><span class="line"></span><br><span class="line">   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)</span><br><span class="line">  ]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>要修改的地方只有标签数量那一块，与先前创建的训练数据文件中的值一致。其余地方不用修改。</p>
<h3 id="3-4-修改train-py程序"><a href="#3-4-修改train-py程序" class="headerlink" title="3.4 修改train.py程序"></a>3.4 修改train.py程序</h3><p>大概在文件的434行，可以对训练时的参数进行修改，包括是否采用预训练参数、先前创建的训练参数文件名称、训练次数等等。部分参数解释如下：</p>
<ul>
<li>–weights：预训练模型，它可以保留原有的训练权重继续训练</li>
<li>–cfg：模型参数文件</li>
<li>–data：数据参数文件</li>
<li>–epochs：训练次数</li>
<li>–batch-size：每次训练的图像数量，越大训练的越快，但是越消耗显存</li>
<li>–imgsz：训练输入图片大小，yolo会根据这个参数压缩图片，越大训练效果越好，但是越消耗显存</li>
<li>–devices：训练时使用的设备，数字表示GPU号，用CPU则改为’cpu’</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">def parse_opt(known=False):</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(&#x27;--weights&#x27;, type=str, default=ROOT / &#x27;yolov5s.pt&#x27;, help=&#x27;initial weights path&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--cfg&#x27;, type=str, default=ROOT / &#x27;models/yolov5s_ball.yaml&#x27;, help=&#x27;model.yaml path&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--data&#x27;, type=str, default=ROOT / &#x27;data/ball.yaml&#x27;, help=&#x27;dataset.yaml path&#x27;) ###</span><br><span class="line">    parser.add_argument(&#x27;--hyp&#x27;, type=str, default=ROOT / &#x27;data/hyps/hyp.scratch.yaml&#x27;, help=&#x27;hyperparameters path&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--epochs&#x27;, type=int, default=500)</span><br><span class="line">    parser.add_argument(&#x27;--batch-size&#x27;, type=int, default=16, help=&#x27;total batch size for all GPUs&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--imgsz&#x27;, &#x27;--img&#x27;, &#x27;--img-size&#x27;, type=int, default=640, help=&#x27;train, val image size (pixels)&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--rect&#x27;, action=&#x27;store_true&#x27;, help=&#x27;rectangular training&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--resume&#x27;, nargs=&#x27;?&#x27;, const=True, default=False, help=&#x27;resume most recent training&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--nosave&#x27;, action=&#x27;store_true&#x27;, help=&#x27;only save final checkpoint&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--noval&#x27;, action=&#x27;store_true&#x27;, help=&#x27;only validate final epoch&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--noautoanchor&#x27;, action=&#x27;store_true&#x27;, help=&#x27;disable autoanchor check&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--evolve&#x27;, type=int, nargs=&#x27;?&#x27;, const=300, help=&#x27;evolve hyperparameters for x generations&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--bucket&#x27;, type=str, default=&#x27;&#x27;, help=&#x27;gsutil bucket&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--cache&#x27;, type=str, nargs=&#x27;?&#x27;, const=&#x27;ram&#x27;, help=&#x27;--cache images in &quot;ram&quot; (default) or &quot;disk&quot;&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--image-weights&#x27;, action=&#x27;store_true&#x27;, help=&#x27;use weighted image selection for training&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--device&#x27;, default=&#x27;0&#x27;, help=&#x27;cuda device, i.e. 0 or 0,1,2,3 or cpu&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--multi-scale&#x27;, action=&#x27;store_true&#x27;, help=&#x27;vary img-size +/- 50%%&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--single-cls&#x27;, action=&#x27;store_true&#x27;, help=&#x27;train multi-class data as single-class&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--adam&#x27;, action=&#x27;store_true&#x27;, help=&#x27;use torch.optim.Adam() optimizer&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--sync-bn&#x27;, action=&#x27;store_true&#x27;, help=&#x27;use SyncBatchNorm, only available in DDP mode&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--workers&#x27;, type=int, default=8, help=&#x27;maximum number of dataloader workers&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--project&#x27;, default=ROOT / &#x27;runs/train&#x27;, help=&#x27;save to project/name&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--name&#x27;, default=&#x27;exp&#x27;, help=&#x27;save to project/name&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--exist-ok&#x27;, action=&#x27;store_true&#x27;, help=&#x27;existing project/name ok, do not increment&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--quad&#x27;, action=&#x27;store_true&#x27;, help=&#x27;quad dataloader&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--linear-lr&#x27;, action=&#x27;store_true&#x27;, help=&#x27;linear LR&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--label-smoothing&#x27;, type=float, default=0.0, help=&#x27;Label smoothing epsilon&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--patience&#x27;, type=int, default=100, help=&#x27;EarlyStopping patience (epochs without improvement)&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--freeze&#x27;, type=int, default=0, help=&#x27;Number of layers to freeze. backbone=10, all=24&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--save-period&#x27;, type=int, default=-1, help=&#x27;Save checkpoint every x epochs (disabled if &lt; 1)&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--local_rank&#x27;, type=int, default=-1, help=&#x27;DDP parameter, do not modify&#x27;)</span><br><span class="line"></span><br><span class="line">    # Weights &amp; Biases arguments</span><br><span class="line">    parser.add_argument(&#x27;--entity&#x27;, default=None, help=&#x27;W&amp;B: Entity&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--upload_dataset&#x27;, action=&#x27;store_true&#x27;, help=&#x27;W&amp;B: Upload dataset as artifact table&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--bbox_interval&#x27;, type=int, default=-1, help=&#x27;W&amp;B: Set bounding-box image logging interval&#x27;)</span><br><span class="line">    parser.add_argument(&#x27;--artifact_alias&#x27;, type=str, default=&#x27;latest&#x27;, help=&#x27;W&amp;B: Version of dataset artifact to use&#x27;)</span><br><span class="line"></span><br><span class="line">    opt = parser.parse_known_args()[0] if known else parser.parse_args()</span><br><span class="line">    return opt</span><br></pre></td></tr></table></figure>

<h3 id="3-5-训练模型"><a href="#3-5-训练模型" class="headerlink" title="3.5 训练模型"></a>3.5 训练模型</h3><p>当做好以上工作后，可以直接在终端运行python3 train.py进行数据集训练。</p>
<p><strong>训练过程可视化：</strong></p>
<p>在yolo.py程序中找到<br><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/14.png" alt="查看"></p>
<p>将该部分取消注释。</p>
<p>在yolov5文件夹终端下输入：tensorboard –logdir&#x3D;.&#x2F;runs，然后在浏览器访问<a href="http://localhost:6006/%E5%8D%B3%E5%8F%AF%E6%9F%A5%E7%9C%8B%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E3%80%82">http://localhost:6006/即可查看训练过程。</a><br><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/15.png" alt="查看"></p>
<h3 id="3-6-训练结果"><a href="#3-6-训练结果" class="headerlink" title="3.6 训练结果"></a>3.6 训练结果</h3><p>训练后的结果会被保存在runs&#x2F;train&#x2F;exp_xx下<br><img src="/image/yolov5%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/results.png" alt="查看"></p>
<ul>
<li>weights文件夹下保存训练的模型参数，包括best.pt和last.pt</li>
<li>result.png表示训练效果：<ul>
<li>Box：</li>
</ul>
</li>
</ul>
<p>将模型导出为torchscripts和onnx模型（可以将训练好的best.pt文件放到yolov5目录下，或者改为runs&#x2F;train&#x2F;exp_xx&#x2F;weights&#x2F;best.pt）：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">python3 export.py --weight best.pt --include torchscript onnx</span><br></pre></td></tr></table></figure>

<h2 id="4-应用与检测"><a href="#4-应用与检测" class="headerlink" title="4.应用与检测"></a>4.应用与检测</h2>]]></content>
      <categories>
        <category>教程</category>
        <category>技能学习</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>YOLOv5</tag>
      </tags>
  </entry>
</search>
